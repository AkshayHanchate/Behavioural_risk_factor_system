{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7040ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Bucket: gcf-v2-sources-892122457228-us-central1>, <Bucket: gcf-v2-uploads-892122457228-us-central1>, <Bucket: on-prem-ingestion-data>]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable within the notebook\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"top-suprstate-438620-h9-7a2effbc371a.json\"\n",
    "\n",
    "# Now proceed with Google Cloud Storage client initialization\n",
    "from google.cloud import storage\n",
    "\n",
    "# Instantiate a Google Cloud Storage client\n",
    "client = storage.Client()\n",
    "\n",
    "# List the buckets in your project\n",
    "buckets = list(client.list_buckets())\n",
    "print(buckets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca21895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunks: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}\n",
      "Chunk 1 already exists in GCS. Skipping...\n",
      "Chunk 2 already exists in GCS. Skipping...\n",
      "Chunk 3 already exists in GCS. Skipping...\n",
      "Chunk 4 already exists in GCS. Skipping...\n",
      "Chunk 5 already exists in GCS. Skipping...\n",
      "Chunk 6 already exists in GCS. Skipping...\n",
      "Chunk 7 already exists in GCS. Skipping...\n",
      "Chunk 8 already exists in GCS. Skipping...\n",
      "Chunk 9 already exists in GCS. Skipping...\n",
      "Chunk 10 already exists in GCS. Skipping...\n",
      "Chunk 11 already exists in GCS. Skipping...\n",
      "Chunk 12 already exists in GCS. Skipping...\n",
      "Chunk 13 already exists in GCS. Skipping...\n",
      "Fetching page 14...\n",
      "Uploading page 14 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_14_20241019_160028.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 15...\n",
      "Uploading page 15 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_15_20241019_160052.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 16...\n",
      "Uploading page 16 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_16_20241019_160116.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 17...\n",
      "Uploading page 17 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_17_20241019_160140.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 18...\n",
      "Uploading page 18 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_18_20241019_160204.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 19...\n",
      "Uploading page 19 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_19_20241019_160227.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 20...\n",
      "Uploading page 20 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_20_20241019_160250.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 21...\n",
      "Uploading page 21 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_21_20241019_160313.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 22...\n",
      "Uploading page 22 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_22_20241019_160338.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 23...\n",
      "Uploading page 23 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_23_20241019_160400.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 24...\n",
      "Uploading page 24 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_24_20241019_160422.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 25...\n",
      "Uploading page 25 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_25_20241019_160444.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 26...\n",
      "Uploading page 26 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_26_20241019_160506.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 27...\n",
      "Uploading page 27 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_27_20241019_160528.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 28...\n",
      "Uploading page 28 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_28_20241019_160550.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 29...\n",
      "Uploading page 29 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_29_20241019_160611.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 30...\n",
      "Uploading page 30 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_30_20241019_160633.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 31...\n",
      "Uploading page 31 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_31_20241019_160655.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 32...\n",
      "Uploading page 32 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_32_20241019_160717.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 33...\n",
      "Uploading page 33 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_33_20241019_160745.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 34...\n",
      "Uploading page 34 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_34_20241019_160813.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 35...\n",
      "Uploading page 35 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_35_20241019_160836.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 36...\n",
      "Uploading page 36 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_36_20241019_160906.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 37...\n",
      "Uploading page 37 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_37_20241019_160934.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 38...\n",
      "Uploading page 38 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_38_20241019_161013.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 39...\n",
      "Uploading page 39 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_39_20241019_161037.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n",
      "Fetching page 40...\n",
      "Uploading page 40 to GCS...\n",
      "Uploaded 2022_data_ingestion/data_page_40_20241019_161113.csv to GCS.\n",
      "Waiting for 3 minutes before processing the next chunk...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up GCS bucket and folder details\n",
    "BUCKET_NAME = 'on-prem-ingestion-data'\n",
    "GCS_FOLDER = '2022_data_ingestion/'  # Folder for 2022 data chunks\n",
    "\n",
    "# Set up GCS client\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(BUCKET_NAME)\n",
    "\n",
    "# Function to fetch data from the API\n",
    "def fetch_data(page, page_size, api_key):\n",
    "    url = f\"http://127.0.0.1:5000/data?page={page}&page_size={page_size}&api_key={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Return data in JSON format\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch data from API: {response.status_code}\")\n",
    "\n",
    "# Function to check which chunks have been uploaded to GCS\n",
    "def get_uploaded_chunks():\n",
    "    blobs = bucket.list_blobs(prefix=GCS_FOLDER)\n",
    "    uploaded_pages = set()\n",
    "    for blob in blobs:\n",
    "        # Extract the page number from the file name: \"data_page_X_YYYYMMDD_HHMMSS.csv\"\n",
    "        filename = blob.name.split(\"/\")[-1]\n",
    "        if \"data_page_\" in filename:\n",
    "            # Extract the page number\n",
    "            page_num = filename.split(\"_\")[2]  # Get the page number\n",
    "            uploaded_pages.add(int(page_num))\n",
    "    return uploaded_pages\n",
    "\n",
    "# Function to upload data to GCS with a timestamp in the file name\n",
    "def upload_to_gcs(data, page):\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create local file path with timestamp\n",
    "    file_name_local = f\"/tmp/data_page_{page}_{timestamp}.csv\"\n",
    "    \n",
    "    # Save the DataFrame to a CSV file locally\n",
    "    df.to_csv(file_name_local, index=False)\n",
    "    \n",
    "    # Create the file name with timestamp for GCS\n",
    "    file_name_gcs = f\"{GCS_FOLDER}data_page_{page}_{timestamp}.csv\"\n",
    "    blob = bucket.blob(file_name_gcs)\n",
    "    blob.upload_from_filename(file_name_local)\n",
    "    \n",
    "    print(f\"Uploaded {file_name_gcs} to GCS.\")\n",
    "\n",
    "# Ingest data in chunks from the API to GCS\n",
    "def ingest_data(api_key, total_pages, page_size):\n",
    "    # Get a set of already uploaded chunks from GCS\n",
    "    uploaded_pages = get_uploaded_chunks()\n",
    "    print(f\"Uploaded chunks: {uploaded_pages}\")\n",
    "\n",
    "    for page in range(1, total_pages + 1):\n",
    "        # If this chunk has already been uploaded, skip it\n",
    "        if page in uploaded_pages:\n",
    "            print(f\"Chunk {page} already exists in GCS. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Fetching page {page}...\")\n",
    "        try:\n",
    "            data = fetch_data(page, page_size, api_key)\n",
    "            if data:\n",
    "                print(f\"Uploading page {page} to GCS...\")\n",
    "                upload_to_gcs(data, page)\n",
    "                \n",
    "                # Sleep for 3 minutes to simulate real-time ingestion\n",
    "                print(\"Waiting for 3 minutes before processing the next chunk...\")\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(f\"No data found for page {page}. Skipping...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page {page}: {e}\")\n",
    "            break  # Stop if there is any issue with the current page\n",
    "\n",
    "# Define parameters\n",
    "API_KEY = 'Accessgranted@4463'\n",
    "TOTAL_PAGES = 40  # You can adjust based on the number of records / page_size\n",
    "PAGE_SIZE = 10000   # Number of records per chunk\n",
    "\n",
    "# Run the ingestion process\n",
    "ingest_data(API_KEY, TOTAL_PAGES, PAGE_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd07d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
