{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec1dc802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Bucket: gcf-v2-sources-892122457228-us-central1>, <Bucket: gcf-v2-uploads-892122457228-us-central1>, <Bucket: on-prem-ingestion-data>]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable within the notebook\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"top-suprstate-438620-h9-7a2effbc371a.json\"\n",
    "\n",
    "# Now proceed with Google Cloud Storage client initialization\n",
    "from google.cloud import storage\n",
    "\n",
    "# Instantiate a Google Cloud Storage client\n",
    "client = storage.Client()\n",
    "\n",
    "# List the buckets in your project\n",
    "buckets = list(client.list_buckets())\n",
    "print(buckets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "794cc14d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_ingestion/2019_data_chunk_1_20241018_201402.csv uploaded to GCS successfully!\n",
      "2019_ingestion/2019_data_chunk_2_20241018_201426.csv uploaded to GCS successfully!\n",
      "2019_ingestion/2019_data_chunk_3_20241018_201452.csv uploaded to GCS successfully!\n",
      "2019_ingestion/2019_data_chunk_4_20241018_201516.csv uploaded to GCS successfully!\n",
      "2019_ingestion/2019_data_chunk_5_20241018_201544.csv uploaded to GCS successfully!\n",
      "2019_ingestion/2019_data_chunk_6_20241018_201616.csv uploaded to GCS successfully!\n",
      "2019_ingestion/2019_data_chunk_7_20241018_201642.csv uploaded to GCS successfully!\n",
      "2019_ingestion/2019_data_chunk_8_20241018_201709.csv uploaded to GCS successfully!\n",
      "2019_ingestion/2019_data_chunk_9_20241018_201738.csv uploaded to GCS successfully!\n",
      "2019_ingestion/2019_data_chunk_10_20241018_201803.csv uploaded to GCS successfully!\n",
      "2019_ingestion/2019_data_chunk_11_20241018_201826.csv uploaded to GCS successfully!\n",
      "All chunks uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to check if a file already exists in GCS\n",
    "def check_if_blob_exists(bucket_name, file_name):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "    return blob.exists()\n",
    "\n",
    "# Function to upload chunk of data to GCS\n",
    "def upload_chunk_to_gcs(local_file, bucket_name, file_name_in_gcs):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name_in_gcs)\n",
    "    \n",
    "    # Upload the file to GCS\n",
    "    blob.upload_from_filename(local_file)\n",
    "    print(f'{file_name_in_gcs} uploaded to GCS successfully!')\n",
    "\n",
    "# Main function for chunking and uploading data\n",
    "def chunk_and_upload(file_path, bucket_name, chunk_size_min=30000, chunk_size_max=50000, wait_time=15):\n",
    "    # Load the CSV data\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Calculate total rows in the CSV\n",
    "    total_rows = data.shape[0]\n",
    "    start = 0\n",
    "    chunk_num = 1\n",
    "\n",
    "    # Define the folder name for GCS (this will create a \"folder\" in GCS)\n",
    "    folder_name_in_gcs = '2019_ingestion/'\n",
    "\n",
    "    # Loop through the data, processing random chunks\n",
    "    while start < total_rows:\n",
    "        # Randomly choose a chunk size between 30,000 and 50,000\n",
    "        chunk_size = random.randint(chunk_size_min, chunk_size_max)\n",
    "        end = min(start + chunk_size, total_rows)\n",
    "\n",
    "        # Extract the chunk of data\n",
    "        data_chunk = data[start:end]\n",
    "\n",
    "        # Add a timestamp to make the file name unique\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        file_name_local = f\"2019_data_chunk_{chunk_num}_{timestamp}.csv\"  # Save locally\n",
    "        file_name_in_gcs = f\"{folder_name_in_gcs}2019_data_chunk_{chunk_num}_{timestamp}.csv\"  # Path in GCS\n",
    "\n",
    "        # Save the chunk to a temporary CSV file locally\n",
    "        data_chunk.to_csv(file_name_local, index=False)\n",
    "\n",
    "        # Check if the file already exists in GCS\n",
    "        if check_if_blob_exists(bucket_name, file_name_in_gcs):\n",
    "            print(f'{file_name_in_gcs} already exists in GCS. Skipping upload.')\n",
    "        else:\n",
    "            # Upload the chunk to GCS\n",
    "            upload_chunk_to_gcs(file_name_local, bucket_name, file_name_in_gcs)\n",
    "\n",
    "        # Remove the local file after upload to save space\n",
    "        os.remove(file_name_local)\n",
    "\n",
    "        # Increment the start position for the next chunk\n",
    "        start = end\n",
    "        chunk_num += 1\n",
    "\n",
    "        # Wait for the specified time (15 seconds) before uploading the next chunk\n",
    "        time.sleep(wait_time)  # Time is in seconds (15 seconds)\n",
    "\n",
    "    print(\"All chunks uploaded successfully!\")\n",
    "\n",
    "# Set the path to your local CSV file and GCS bucket\n",
    "csv_file_path = '2019.csv'  # Update with the actual path\n",
    "bucket_name = 'on-prem-ingestion-data'  # Replace with your GCS bucket name\n",
    "\n",
    "# Call the function to start chunking and uploading\n",
    "chunk_and_upload(csv_file_path, bucket_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df7600e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "print(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b98342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
